{
    "collab_server" : "",
    "contents" : "#load data in project\n#data <- load(\"/Users/dsalimian/Documents/School/Master/Multivariate Statistics/Week1/supermarket1996.RData\")\n\n# a: program ridge regression as an R function\n\nridgeRegression <- function(X, y, lambda, standardize = TRUE, intercept = TRUE){\n  # NOTE: maybe first test for heteroscedasticity and use dependent on the results either the normal or hetero ridge approach\n  \n  # if we want to standardize our input, then do so here\n  if(standardize){\n    # standardize the columns to z-score\n    X_mean <- apply(X, 2, mean) #note this is a vector containing all column means of matrix X\n    X_stdev <- apply(X, 2, sd) # note this is a vector containing all column standard deviations of matrix X\n    X_stnd <- scale(X,X_mean,X_stdev) # this contains now the z-scores for each column  \n  }\n  \n  # if we do not want any intercept..\n  if(!intercept){\n    # make sure that our (in)dependent variable sums to 0. Hence remove the mean from each observation\n    y_mean <- mean(y)\n    y_stnd <- y-y_mean\n    \n    # only de-mean X when it isn't standardized yet\n    if(!standardize){\n      X_mean <- apply(X, 2, mean) #note this is a vector containing all column means of matrix X\n      X_stnd <- scale(X,X_mean, FALSE)\n    }\n  }else{\n    y_stnd <- y # when we do not de-mean, and thus leave the intercept, we need to rename our dependent var for the rest of the code to work\n  }\n  \n  if(!standardize && intercept){\n    X_stnd <- as.matrix(X) # when we do not standardize, and leave the intercept, we need to rename our input var for the rest of the code to work\n  }\n  \n  y_ext <- c(y_stnd, rep(0, dim(X_stnd)[2]))\n    \n  iden <- diag(dim(X_stnd)[2])*sqrt(lambda)\n  X_ext <- rbind(X_stnd, iden)\n  X_ext <- as.data.frame(X_ext)\n  colnames(X_ext) <- names(X)\n  \n  data <- as.data.frame(cbind(X_ext, y_ext))\n  model <- lm(y_ext~. -1,data)\n  \n  A <- as.numeric((model$residuals %*% model$residuals)/(dim(X)[1]-dim(X)[2])) * solve(t(X_stnd) %*% X_stnd + lambda * diag(dim(X_stnd)[2]))  %*%\n                t(X_stnd) %*% X_stnd %*% solve(t(X_stnd) %*% X_stnd + lambda * diag(dim(X_stnd)[2]))\n  \n  b_std <- diag(A)^(1/2)\n  \n  return <- list(model$coefficients, b_std)\n}\n    \n  \n  # # since lambda is given, we can proceed and calculate the beta_ridged now\n  # b_ridged <- solve(t(X_stnd) %*% X_stnd + lambda * diag(dim(X_stnd)[2])) %*% t(X_stnd) %*% y\n  # \n  # # using the esitmates paramters, we can calculate our y_hat values\n  # y_hat <- X_stnd %*% b_ridged\n  # \n  # # now calculate the standard deviation of the estimated parameters b_ridged\n  # estimation_error <- y_stnd-y_hat\n  # error_variance <- (t(estimation_error)%*%estimation_error)/(dim(X_stnd)[1]-dim(X_stnd)[2])\n  # error_variance <- as.numeric(error_variance)\n  # \n  # b_ridged_var <- error_variance * solve(t(X_stnd) %*% X_stnd + lambda * diag(dim(X_stnd)[2])) %*% t(X_stnd) %*%\n  #   X_stnd %*% solve(t(X_stnd) %*% X_stnd + lambda * diag(dim(X_stnd)[2]))\n  # \n  # # calculate the degrees of freedom\n  # hat_ridge <- X_stnd %*% solve(t(X_stnd) %*% X_stnd + lambda * diag(dim(X_stnd)[2])) %*% t(X_stnd)\n  # df_ridge <- sum(diag(hat_ridge)) # this is taking the trace of the hat_ridge variable\n  # \n  # # calculate the AIC value\n  # AIC <- dim(X_stnd)[2] * log(t(estimation_error)%*%estimation_error) + 2 * df_ridge\n  # AIC <- as.numeric(AIC)\n  # \n  # # calculate the AIC value\n  # BIC <- dim(X_stnd)[2] * log(t(estimation_error)%*%estimation_error) + 2 * df_ridge * log(dim(X_stnd)[2])\n  # BIC <- as.numeric(BIC)\n  # \n  # # calculate the correlation coefficient of y and y_hat\n  # R = cov(y,y_hat)/(sd(y) * sd(y_hat))\n  # R = as.numeric(R)\n  # R_sq = R^2\n  # \n  # result = list(b_ridge=b_ridged, b_ridged_var=b_ridged_var, estimation_error_sum=sum(estimation_error), AIC=AIC, BIC=BIC, R_sq=R_sq, df_ridge=df_ridge)\n  # return(result)\n  \n# }\n\nX <- supermarket1996[,6:50]\ny <- supermarket1996$GROCCOUP_sum\nlambda = 1\nmodel1 = ridgeRegression(X,y,lambda, standardize = FALSE, intercept = TRUE) # answer to a) 1\nmodel2 = ridgeRegression(X,y,lambda, standardize = FALSE, intercept = FALSE) # answer to a) 2\nmodelstd <- lm.ridge(y~.-1, cbind(X,y))\n\nmodel3 = ridgeRegression(X,y,lambda, standardize = TRUE, intercept = FALSE) # answer to c)\n\nprint('b_ridged is as follows:')\nprint(model1$b_ridged)\nprint('variance of b_ridged is as follows:')\nprint(model1$b_ridged_var)\npaste('the sum of the estimation errors is', model1$estimation_error_sum)\npaste('the AIC value is', model1$AIC)\npaste('the BIC value is', model1$BIC)\npaste('R^2 value is ', model1$R_sq)\npaste('the effective degrees of freedom is ', model1$df_ridge)\n\n# now get the results using a package that is available for R; glmnet\n\n# install.packages('glmnet') # helps installing the package if neccesary\nlibrary('glmnet')\nridge_glmnet = glmnet(as.matrix(X),y, standardize=TRUE, intercept=FALSE )\nridge_glmnet\n\n# install.packages('genridge')\nlibrary('genridge')\nmodel_ridge = ridge(y, X)\n",
    "created" : 1472721347602.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3891214888",
    "id" : "4914C395",
    "lastKnownWriteTime" : 1472727775,
    "last_content_update" : 1472733761265,
    "path" : "C:/Users/Armin/OneDrive/Dokumenty/Projekty R/Multivariate Statistics/Assigment1/AssignmentWeek1.R",
    "project_path" : "AssignmentWeek1.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}